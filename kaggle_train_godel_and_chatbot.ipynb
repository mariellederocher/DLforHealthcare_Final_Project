{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b56145c",
   "metadata": {},
   "source": [
    "# Godel Model Fine-Tuning + Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280dd23",
   "metadata": {},
   "source": [
    "This notebook is written to be run using a Kaggle notebook running a GPU T4 as an accelerator. \n",
    "\n",
    "It contains both the code for fine tuning Godel and for the chatbot loop, so that the model is already loaded for the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8768d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d39bb",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ab8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_doctor_data(path=\"/kaggle/input/mts-dialog-qa-dataset/question_answer_dataset.jsonl\"):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                data_entry = json.loads(line)\n",
    "                symptom = data_entry[\"finding\"]\n",
    "                emotion = data_entry[\"emote\"]\n",
    "                question = data_entry[\"doctor_q\"]\n",
    "                prev_answer = data_entry[\"prev_patient_a\"]\n",
    "                input_text = f\"Symptom: {symptom}; Previous Patient Response: {prev_answer}\"\n",
    "                output_text = f\"{question} ({emotion})\"\n",
    "                data.append({\"input\": str(input_text), \"output\": str(output_text)})\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()}\")\n",
    "    return Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1669dae",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model_name=\"microsoft/GODEL-v1_1-base-seq2seq\", output_dir=\"./godel_finetuned\"):\n",
    "    dataset = load_doctor_data()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess(example):\n",
    "        inputs = tokenizer(example[\"input\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        targets = tokenizer(example[\"output\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=3,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_dir='./logs',\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "fine_tune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e3ed5",
   "metadata": {},
   "source": [
    "## Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86c991",
   "metadata": {},
   "source": [
    "This is the same code from MedicalKnowledgeBase.py, pasted for easy use on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "class MedicalKnowledgeBase:\n",
    "    def __init__(self, kb_json_file=None):\n",
    "        self.df = pd.DataFrame({\n",
    "            \"diagnosis\" : [],\n",
    "            \"finding\" : [],\n",
    "            \"evoking_strength\" : [], \n",
    "            \"frequency\" : []\n",
    "        })\n",
    "        self.diagnosis_list = []\n",
    "        self.script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        if kb_json_file:\n",
    "            with open(kb_json_file) as f: \n",
    "                data = json.load(f)\n",
    "            for diagnosis in data:\n",
    "                for finding in data[diagnosis]:\n",
    "                    self.add_entry(diagnosis, finding, 1, data[diagnosis][finding])\n",
    "\n",
    "    def get_kb(self):\n",
    "        print(self.df.head())\n",
    "    \n",
    "    def add_entry(self, diagnosis, finding, evoking_strength, frequency):\n",
    "        self.df.loc[len(self.df)] = [diagnosis, finding, evoking_strength, frequency]\n",
    "        self.diagnosis_list = self.df['diagnosis'].unique()\n",
    "\n",
    "\n",
    "    \n",
    "    def get_diagnoses_for_findings(self, findings, neg_findings=[], *, match_req=0):\n",
    "        #if no findings need to be excluded and diagnosis is not required to match findings, no need to loop\n",
    "        if len(neg_findings) == 0 and match_req == 0:\n",
    "            return self.diagnosis_list\n",
    "        \n",
    "        df = self.df\n",
    "        invalid_diagnoses = df[df[\"finding\"].isin(neg_findings)]['diagnosis'].tolist()\n",
    "\n",
    "        if match_req == 0:\n",
    "            possible_diagnoses = self.diagnosis_list\n",
    "        else:\n",
    "            match_df = df[df[\"finding\"].isin(findings)]['diagnosis']\n",
    "            counts = match_df.groupby(\"Employee_Name\").size()\n",
    "            possible_diagnoses = counts[counts >= match_req].index.to_list() \n",
    "              \n",
    "        return set(possible_diagnoses).difference(invalid_diagnoses)\n",
    "    \n",
    "\n",
    "    def suggest_next_finding(self, current_findings, neg_findings=[], *, match_req=0):\n",
    "        valid_diagnoses = self.get_diagnoses_for_findings(current_findings, neg_findings, match_req=match_req)\n",
    "        df = self.df[self.df['diagnosis'].isin(valid_diagnoses)]\n",
    "\n",
    "        # find common findings not yet observed\n",
    "        candidate_scores = {}\n",
    "        for _, row in df.iterrows():\n",
    "            if (row['finding'] not in current_findings and row['finding'] not in neg_findings):\n",
    "                score = row[\"evoking_strength\"] * row[\"frequency\"]\n",
    "                candidate_scores[row['finding']] = candidate_scores.get(row['finding'], 0) + score\n",
    "\n",
    "        # return the best finding\n",
    "        if not candidate_scores:\n",
    "            return None\n",
    "        return max(candidate_scores, key=candidate_scores.get)\n",
    "\n",
    "\n",
    "    def get_random_finding(self, current_findings=[], neg_findings=[]):\n",
    "        finding = self.df.sample(n=1)['finding'].values[0]\n",
    "        while finding in current_findings or finding in neg_findings:\n",
    "            finding = self.df.sample(n=1)['finding'].values[0]\n",
    "        return finding\n",
    "    \n",
    "    def get_random_findings(self, num_findings):\n",
    "        return random.sample(sorted(self.df['finding'].unique()), num_findings)\n",
    "    \n",
    "\n",
    "    def save_kb_as_csv(self, filename=\"outputs/medical_kb.csv\"):\n",
    "        filename = os.path.join(self.script_dir, filename)\n",
    "        self.df.to_csv(filename, index=False)\n",
    "\n",
    "    def load_kb(self, filename=\"outputs/medical_kb.csv\"):\n",
    "        filename = os.path.join(self.script_dir, filename)\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.diagnosis_list = self.df['diagnosis'].unique()\n",
    "\n",
    "kb = MedicalKnowledgeBase(\"mimic_4_kb_w_freq.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f4992",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ceaa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"godel_finetuned\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"godel_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot loop\n",
    "def generate_question(symptom, emotion=\"neutral\"):\n",
    "    prompt = f\"Emotion: {emotion}; Symptom: {symptom}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"(\")[0].strip()\n",
    "\n",
    "def chatbot_loop():\n",
    "    print(\"Chatbot: Hello, I'm going to ask you some questions about your symptoms.\")\n",
    "    current_findings = []\n",
    "    neg_findings = []\n",
    "    next_finding = kb.get_random_finding()\n",
    "    while len(current_findings) + len(neg_findings) < 10:\n",
    "\n",
    "        # emotion = random.choice([\"ne\"])\n",
    "        question = generate_question(next_finding)\n",
    "\n",
    "        print(f\"Chatbot: {question}\")\n",
    "        user_input = input(\"You: \")\n",
    "        if any(word in user_input.lower() for word in [\"yes\", \"yeah\", \"y\", \"i have\", \"sure\"]):\n",
    "            current_findings.append(next_finding)\n",
    "        else:\n",
    "            neg_findings.append(next_finding)\n",
    "\n",
    "        # Suggest next finding\n",
    "        next_finding = kb.suggest_next_finding(current_findings, neg_findings)\n",
    "\n",
    "    print(\"Chatbot: Thank you for your time.\")\n",
    "\n",
    "chatbot_loop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
